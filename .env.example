# Azure AI / Microsoft Foundry Configuration
# Get these from your Microsoft Foundry project

# Project endpoint (e.g., https://<resource>.services.ai.azure.com/api/projects/<project>)
AZURE_AI_PROJECT_ENDPOINT=<project endpoint>

# Model deployment name (e.g., gpt-5.2-chat, gpt-5.2, gpt-4o-mini)
AZURE_AI_MODEL_DEPLOYMENT_NAME=<deployment name>

# Todo API URL - JSONPlaceholder API for todo data
TODO_API_URL=https://jsonplaceholder.typicode.com/todos

# Optional: Port for this service (default: 8080)
PORT=8080

# Optional: Service name for OpenTelemetry tracing (default: todo-agent)
# OTEL_SERVICE_NAME=todo-agent

# Application Insights export
#
# Agent Framework emits OpenTelemetry spans, but you still need to configure an
# exporter/backend.
#
# To send traces to Application Insights, set your connection string here:
# APPLICATIONINSIGHTS_CONNECTION_STRING=InstrumentationKey=...;IngestionEndpoint=...
#
# Alternatively, you can send to any OTLP-compatible backend by setting OTEL
# exporter env vars (example for a local OTLP collector / Aspire dashboard):
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
# ENABLE_CONSOLE_EXPORTERS=true
